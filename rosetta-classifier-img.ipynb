{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # per definire il livello di log: non stampa info e warning, ma solo gli errori\n",
    "import csv #modulo per i csv\n",
    "from PIL import Image #image fornisce una classe per rappresentare un immagine pil. Pil è la libreria per la gestione delle immagini di python \n",
    "import numpy as np #fornisce le librerie come sklearn e scipy. Fornisce oggetti multidimensionali. Necessario per il ML\n",
    "from sklearn.model_selection import train_test_split #sklearn libreria per ML. Fornisce modelli per classificazione, regressione, clustering, ecc. Train_test_split splitta gli array o le matrici in sottosezioni di train e test\n",
    "from keras.preprocessing.image import img_to_array #keras è una libreria per ML. img_to_array converte una immagine PIL in un array Numpy\n",
    "from keras.preprocessing.image import load_img # load_img carica una immagine nel formato PIL\n",
    "from keras.models import Sequential #Sequential è una classe che fornisce le feature di training e inferenza \n",
    "from keras.layers import Dense, Conv2D, Flatten,Dropout, MaxPooling2D, AveragePooling2D \n",
    "# Dense layer: è il normale strato di rete neurale profondamente connesso. È il livello più comune e utilizzato di frequente. Il Dense Layer esegue l'operazione seguente sull'input e restituisce l'output.\n",
    "# Conv2D layer: questo livello crea un kernel di convoluzione che è convoluto con l'input del livello per produrre un tensore di output. \n",
    "# Flatten layer: appiattisce l'input. \n",
    "# Dropout layer: imposta casualmente le unità di input su 0 con una frequenza di frequenza ad ogni passaggio durante il tempo di training, il che aiuta a prevenire il sovradattamento\n",
    "# MaxPooling2D  layer: Operazione di pooling massimo per i dati spaziali 2D. Sottocampiona l'input lungo le sue dimensioni spaziali (altezza e larghezza) prendendo il valore massimo su una finestra di input (di dimensione definita da pool_size) per ogni canale dell'input. La finestra viene spostata di passi lungo ogni dimensione.\n",
    "# AveragePooling2D layer: Sottocampiona l'input lungo le sue dimensioni spaziali (altezza e larghezza) prendendo il valore medio su una finestra di input (di dimensione definita da pool_size) per ogni canale dell'input. La finestra viene spostata di passi lungo ogni dimensione.\n",
    "from sklearn.preprocessing import LabelBinarizer # Binarizza le etichette in modo uno contro tutti.In scikit-learn sono disponibili diversi algoritmi di regressione e di classificazione binaria. Un modo semplice per estendere questi algoritmi al caso di classificazione multiclasse consiste nell'utilizzare il cosiddetto schema uno contro tutti.Al momento dell'apprendimento, questo consiste semplicemente nell'imparare un regressore o un classificatore binario per classe. In tal modo, è necessario convertire le etichette multiclasse in etichette binarie (appartengono o non appartengono alla classe). LabelBinarizer semplifica questo processo con il metodo di trasformazione.Al momento della previsione, si assegna la classe per la quale il modello corrispondente ha dato la massima confidenza. LabelBinarizer lo rende facile con il metodo inverse_transform.\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier # KerasClassifier: Implementazione dell'API del classificatore scikit-learn per Keras.\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score # RepeatedKFold ripete K-Fold n volte. Può essere utilizzato quando si richiede di eseguire KFold n volte, producendo split diversi in ogni ripetizione.\n",
    "# cross_val_score valuta un punteggio tramite  cross-validation\n",
    "import matplotlib.pyplot as plt #plotta i dati\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def open_image(X_value):\n",
    "    return (1. / 255) * img_to_array(load_img('dataset/'+ X_value))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "x_set = []\n",
    "y_set = []\n",
    "\n",
    "with open('index.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if len(row) > 1:\n",
    "            if row[0] and row[1] and row[2] and row[3]:\n",
    "                    if os.path.isfile(\"dataset/\" + row[0]):\n",
    "                        x_set.append(row[0])\n",
    "                        y_set.append(row[3])\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_set_cat = lb.fit_transform(y_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_set, y_set_cat, test_size=0.33, random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "input_size = open_image(X_train[0]).shape\n",
    "output_size = len(y_set_cat[0])\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    # The first two layers with 32 filters of window size 3x3\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_size, kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "ann_model = KerasClassifier(build_fn=baseline_model, epochs=1, batch_size=16) #batch = quanti esempi prende per volta per aggiornare la rete neurale\n",
    "fin_model = KerasClassifier(build_fn=baseline_model, epochs=1, batch_size=16)\n",
    "X_train_adapt = np.array(list(map(open_image, X_train)))\n",
    "baseline_model().summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfold= RepeatedKFold(n_splits=5, n_repeats=1)\n",
    "results= cross_val_score(ann_model, X_train_adapt, y_train, cv=kfold, n_jobs=1, verbose=1)  # 2 cpus\n",
    "results.mean()  # Mean MSE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "2021-07-23 15:49:55.402299: E tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2021-07-23 15:49:55.402556: E tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "416/416 [==============================] - 17s 17ms/step - loss: 4.4443 - acc: 0.1431\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 2.7683 - acc: 0.2780\n",
      " 89/416 [=====>........................] - ETA: 38s - loss: 8.0479 - acc: 0.1147"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "history = fin_model.fit(X_train_adapt, y_train)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/alby/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:320068)",
      "at w.execute (/home/alby/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:319389)",
      "at w.start (/home/alby/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:315205)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/alby/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:329732)",
      "at async t.CellExecutionQueue.start (/home/alby/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:329272)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test_adapt = np.array(list(map(open_image, X_test)))\n",
    "print(f' - Accuracy: {fin_model.score(X_test_adapt, y_test)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = fin_model.predict(X_test)\n",
    "\n",
    "def map_to_array(x): \n",
    "    final = [0] * len(lb.classes_)\n",
    "    final[x] = 1\n",
    "    return final\n",
    "    \n",
    "y_pred = np.array(list(map(map_to_array, list(y_pred))))\n",
    "conf_matrix = confusion_matrix(y_test, lb.inverse_transform(y_pred), normalize='true', labels=accepted)\n",
    "df_cm = pd.DataFrame(conf_matrix, index=accepted,\n",
    "                     columns=accepted)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"Blues\")\n",
    "plt"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}